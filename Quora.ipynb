{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"C:/Users/admin/Desktop/NLP/Quora Sent analysis/input\"))\n",
    "dir = \"C:/Users/admin/Desktop/NLP/Quora Sent analysis/input\"\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import operator \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length: 1306122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(dir + \"/train.csv\")\n",
    "train_df_len = train_df.shape[0]\n",
    "print('train data length: {}'.format(train_df_len)) # 1306122\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 0 vs 1 = 1225312 vs 80810, 93.81% vs 6.19%\n"
     ]
    }
   ],
   "source": [
    "t0, t1 = len(train_df[train_df.target==0]), len(train_df[train_df.target==1])\n",
    "t0_pct, t1_pct = t0 / train_df_len * 100, t1 / train_df_len * 100\n",
    "print('target 0 vs 1 = {} vs {}, {:.2f}% vs {:.2f}%'.format(t0, t1, t0_pct, t1_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data length: 375806\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>Why do so many women become so rude and arroga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>When should I apply for RV college of engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>What is it really like to be a nurse practitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>Who are entrepreneurs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>Is education really making good people nowadays?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...\n",
       "1  00002bd4fb5d505b9161  When should I apply for RV college of engineer...\n",
       "2  00007756b4a147d2b0b3  What is it really like to be a nurse practitio...\n",
       "3  000086e4b7e1c7146103                             Who are entrepreneurs?\n",
       "4  0000c4c3fbe8785a3090   Is education really making good people nowadays?"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(dir + \"/test.csv\")\n",
    "print('test data length: {}'.format(test_df.shape[0]))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample submission length: 375806\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  0000163e3ea7c7a74cd7           0\n",
       "1  00002bd4fb5d505b9161           0\n",
       "2  00007756b4a147d2b0b3           0\n",
       "3  000086e4b7e1c7146103           0\n",
       "4  0000c4c3fbe8785a3090           0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv(dir + '/sample_submission.csv')\n",
    "print('sample submission length: {}'.format(sample_df.shape[0]))\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions corrections\n",
    "contraction_dict = {\n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, contraction_dict):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([contraction_dict[t] if t in contraction_dict else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special characters\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_dict = {\n",
    "    \"‘\": \"'\",    \"₹\": \"e\",      \"´\": \"'\", \"°\": \"\",         \"€\": \"e\",\n",
    "    \"™\": \"tm\",   \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",        \"—\": \"-\",\n",
    "    \"–\": \"-\",    \"’\": \"'\",      \"_\": \"-\", \"`\": \"'\",        '“': '\"',\n",
    "    '”': '\"',    '“': '\"',      \"£\": \"e\", '∞': 'infinity', 'θ': 'theta',\n",
    "    '÷': '/',    'α': 'alpha',  '•': '.', 'à': 'a',        '−': '-',\n",
    "    'β': 'beta', '∅': '',       '³': '3', 'π': 'pi'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, punct_dict):\n",
    "    for p in punct_dict:\n",
    "        text = text.replace(p, punct_dict[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, contraction_dict, punct, punct_dict):\n",
    "    texts = df.question_text\n",
    "    processed_texts = texts.apply(lambda x: x.lower())\n",
    "    processed_texts = processed_texts.apply(lambda x: clean_contractions(x, contraction_dict))\n",
    "    processed_texts = processed_texts.apply(lambda x: clean_special_chars(x, punct, punct_dict))\n",
    "    processed_texts = processed_texts.apply(lambda x: re.split('\\W+', x))\n",
    "    processed_texts = processed_texts.apply(lambda x: [token for token in x if token not in stopwords])\n",
    "    df['processed_text'] = processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_ROWS_T0 = 639190\n",
    "SAMPLE_ROWS_T1 = 80810\n",
    "df_t0 = train_df[train_df.target==0].sample(SAMPLE_ROWS_T0)\n",
    "df_t1 = train_df[train_df.target==1].sample(SAMPLE_ROWS_T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484879</th>\n",
       "      <td>5ef340417ffdee1f7fb9</td>\n",
       "      <td>What does it mean if my ex acts like he hates ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mean, ex, acts, like, hates, always, looks, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634472</th>\n",
       "      <td>7c417d73cf6e45ac9a47</td>\n",
       "      <td>What is it like working for the Canadian milit...</td>\n",
       "      <td>0</td>\n",
       "      <td>[like, working, canadian, military, trans, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678872</th>\n",
       "      <td>84f348126f2b253cb054</td>\n",
       "      <td>Why is it important to perform system analysis?</td>\n",
       "      <td>0</td>\n",
       "      <td>[important, perform, system, analysis, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860777</th>\n",
       "      <td>a8a436c049b231799c7b</td>\n",
       "      <td>When will my deposited money will be aviable i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[deposited, money, aviable, account, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496088</th>\n",
       "      <td>6124e3ca86664e479bfd</td>\n",
       "      <td>Why does a pure tungsten filament emit a conti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[pure, tungsten, filament, emit, continuous, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "484879  5ef340417ffdee1f7fb9   \n",
       "634472  7c417d73cf6e45ac9a47   \n",
       "678872  84f348126f2b253cb054   \n",
       "860777  a8a436c049b231799c7b   \n",
       "496088  6124e3ca86664e479bfd   \n",
       "\n",
       "                                            question_text  target  \\\n",
       "484879  What does it mean if my ex acts like he hates ...       0   \n",
       "634472  What is it like working for the Canadian milit...       0   \n",
       "678872    Why is it important to perform system analysis?       0   \n",
       "860777  When will my deposited money will be aviable i...       0   \n",
       "496088  Why does a pure tungsten filament emit a conti...       0   \n",
       "\n",
       "                                           processed_text  \n",
       "484879  [mean, ex, acts, like, hates, always, looks, t...  \n",
       "634472  [like, working, canadian, military, trans, per...  \n",
       "678872           [important, perform, system, analysis, ]  \n",
       "860777             [deposited, money, aviable, account, ]  \n",
       "496088  [pure, tungsten, filament, emit, continuous, s...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_t0, contraction_dict, punct, punct_dict)\n",
    "df_t0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>959197</th>\n",
       "      <td>bbedfe8afd5fef69da5c</td>\n",
       "      <td>Do you think a certain percentage of Americans...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, certain, percentage, americans, always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274636</th>\n",
       "      <td>35bff8f8bde326bcd2ba</td>\n",
       "      <td>How did Xi Jinping made Apple sold out their i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[xi, jinping, made, apple, sold, icloud, china...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147599</th>\n",
       "      <td>1cde4cc9bf5c04c4dd0f</td>\n",
       "      <td>If leftists are correct &amp; Trump isn't as rich ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[leftists, correct, trump, rich, claims, pull, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786660</th>\n",
       "      <td>9a1cac9321a23b9a5f05</td>\n",
       "      <td>Why has Brahmin bashing become so common?</td>\n",
       "      <td>1</td>\n",
       "      <td>[brahmin, bashing, become, common, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64128</th>\n",
       "      <td>0c9360bbd317712240ca</td>\n",
       "      <td>Why do foreigners claim that their Filipina wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[foreigners, claim, filipina, wife, beautiful,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "959197  bbedfe8afd5fef69da5c   \n",
       "274636  35bff8f8bde326bcd2ba   \n",
       "147599  1cde4cc9bf5c04c4dd0f   \n",
       "786660  9a1cac9321a23b9a5f05   \n",
       "64128   0c9360bbd317712240ca   \n",
       "\n",
       "                                            question_text  target  \\\n",
       "959197  Do you think a certain percentage of Americans...       1   \n",
       "274636  How did Xi Jinping made Apple sold out their i...       1   \n",
       "147599  If leftists are correct & Trump isn't as rich ...       1   \n",
       "786660          Why has Brahmin bashing become so common?       1   \n",
       "64128   Why do foreigners claim that their Filipina wi...       1   \n",
       "\n",
       "                                           processed_text  \n",
       "959197  [think, certain, percentage, americans, always...  \n",
       "274636  [xi, jinping, made, apple, sold, icloud, china...  \n",
       "147599   [leftists, correct, trump, rich, claims, pull, ]  \n",
       "786660               [brahmin, bashing, become, common, ]  \n",
       "64128   [foreigners, claim, filipina, wife, beautiful,...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df_t1, contraction_dict, punct, punct_dict)\n",
    "df_t1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>Why do so many women become so rude and arroga...</td>\n",
       "      <td>[many, women, become, rude, arrogant, get, lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>When should I apply for RV college of engineer...</td>\n",
       "      <td>[apply, rv, college, engineering, bms, college...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>What is it really like to be a nurse practitio...</td>\n",
       "      <td>[really, like, nurse, practitioner, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>Who are entrepreneurs?</td>\n",
       "      <td>[entrepreneurs, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>Is education really making good people nowadays?</td>\n",
       "      <td>[education, really, making, good, people, nowa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...   \n",
       "1  00002bd4fb5d505b9161  When should I apply for RV college of engineer...   \n",
       "2  00007756b4a147d2b0b3  What is it really like to be a nurse practitio...   \n",
       "3  000086e4b7e1c7146103                             Who are entrepreneurs?   \n",
       "4  0000c4c3fbe8785a3090   Is education really making good people nowadays?   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [many, women, become, rude, arrogant, get, lit...  \n",
       "1  [apply, rv, college, engineering, bms, college...  \n",
       "2              [really, like, nurse, practitioner, ]  \n",
       "3                                  [entrepreneurs, ]  \n",
       "4  [education, really, making, good, people, nowa...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test_df, contraction_dict, punct, punct_dict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, vocab):\n",
    "    for word in texts:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177342\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "df_t1.processed_text.apply(lambda x: build_vocab(x, vocab))\n",
    "df_t0.processed_text.apply(lambda x: build_vocab(x, vocab))\n",
    "test_df.processed_text.apply(lambda x: build_vocab(x, vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'gents',\n",
       " 'scool',\n",
       " 'logarithms',\n",
       " 'overclockable',\n",
       " 'kannads',\n",
       " 'pitter',\n",
       " 'angio',\n",
       " 'longtime',\n",
       " 'cartoon',\n",
       " 'underexplored',\n",
       " 'dalliance',\n",
       " 'machinary',\n",
       " 'mazandaranis',\n",
       " 'giantess',\n",
       " 'whyasansol',\n",
       " 'plcs',\n",
       " 'ardently',\n",
       " 'declaring',\n",
       " 'safex',\n",
       " 'realistically',\n",
       " 'yudkowsky',\n",
       " 'itpin',\n",
       " 'cannikin',\n",
       " 'elastically',\n",
       " 'hsync',\n",
       " 'plr',\n",
       " 'hoodwinked',\n",
       " 'implying',\n",
       " 'kakashi',\n",
       " 'damate',\n",
       " 'umbrical',\n",
       " 'baguio',\n",
       " 'bioweapons',\n",
       " 'forenames',\n",
       " 'avelox',\n",
       " 'purdah',\n",
       " 'starin',\n",
       " 'fwrite',\n",
       " 'enkidu',\n",
       " 'collabarative',\n",
       " 'umbilical',\n",
       " '5p',\n",
       " 'dispensations',\n",
       " 'novgorod',\n",
       " 'quants',\n",
       " '500g',\n",
       " 'agora',\n",
       " 'xat',\n",
       " 'defaulted',\n",
       " 'insurable',\n",
       " 'sizzler',\n",
       " 'aircfaft',\n",
       " 'andres',\n",
       " 'loney',\n",
       " 'keywest',\n",
       " 'disrespectful',\n",
       " 'katyn',\n",
       " 'ba42',\n",
       " 'sewall',\n",
       " 'makut',\n",
       " 'iriana',\n",
       " '13kgs',\n",
       " 'mucinex',\n",
       " 'wildlings',\n",
       " 'manse',\n",
       " 'cinematic',\n",
       " 'maniacs',\n",
       " 'shining',\n",
       " 'winstrol',\n",
       " 'amass',\n",
       " 'g10m',\n",
       " 'hapneed',\n",
       " 'dimarco',\n",
       " 'hwhbl',\n",
       " 'lm337',\n",
       " 'lucent',\n",
       " 'lowlife',\n",
       " 'indivudal',\n",
       " 'lavoy',\n",
       " 'alicyclic',\n",
       " 'zauberberg',\n",
       " 'lakhnaur',\n",
       " 'galsworthy',\n",
       " 'holdouts',\n",
       " 'peninsuald',\n",
       " 'attentional',\n",
       " 'perfer',\n",
       " 'condiments',\n",
       " 'willamette',\n",
       " '1974',\n",
       " 'iditarod',\n",
       " 'panama',\n",
       " 'updare',\n",
       " 'fasttext',\n",
       " 'areata',\n",
       " 'line6',\n",
       " 'rolfing',\n",
       " 'epclusa',\n",
       " 'annoys',\n",
       " 'lth',\n",
       " 'crowfunding',\n",
       " 'unrecognised',\n",
       " 'aido',\n",
       " 'tresses',\n",
       " 'kanyadaan',\n",
       " 'symbolises',\n",
       " 'orchidectomy',\n",
       " 'tyndallization',\n",
       " 'ilma',\n",
       " 'noukari',\n",
       " 'unconsiously',\n",
       " 'crossbreed',\n",
       " '8wx',\n",
       " 'strartups',\n",
       " 'profissional',\n",
       " 'gottfried',\n",
       " 'koovs',\n",
       " 'cide',\n",
       " 'morell',\n",
       " 'unlike',\n",
       " 'bighit',\n",
       " 'rhec',\n",
       " 'allu',\n",
       " 'professor',\n",
       " 'hamburg',\n",
       " 'fogeting',\n",
       " 'dwindles',\n",
       " 'streamlined',\n",
       " 'heir',\n",
       " 'prakasam',\n",
       " 'nondisjunction',\n",
       " '50a2000',\n",
       " 'hosapete',\n",
       " 'backpacked',\n",
       " 'mnist',\n",
       " 'dorne',\n",
       " 'bhagwa',\n",
       " '24250',\n",
       " 'articeship',\n",
       " 'istikhara',\n",
       " '1830',\n",
       " '88000',\n",
       " 'arabiandate',\n",
       " 'approximative',\n",
       " 'tracheid',\n",
       " 'omic',\n",
       " 'dduplicated',\n",
       " 'magill',\n",
       " 'coors',\n",
       " 'infraprojects',\n",
       " '4020',\n",
       " 'kassel',\n",
       " 'concentrating',\n",
       " 'kohana',\n",
       " 'raffles',\n",
       " 'jamb',\n",
       " 'stuents',\n",
       " 'gwadar',\n",
       " 'galler',\n",
       " 'alakli',\n",
       " 'ranh',\n",
       " 'hums',\n",
       " 'av40',\n",
       " 'airstrip',\n",
       " 'choc',\n",
       " '400058',\n",
       " 'cuddler',\n",
       " 'ameritech',\n",
       " 'sandton',\n",
       " 'aroud',\n",
       " 'gbc',\n",
       " 'vi',\n",
       " 'merrily',\n",
       " 'repurposed',\n",
       " 'cabinets',\n",
       " 'propeproper',\n",
       " 'embryonal',\n",
       " 'hettich',\n",
       " 'mongodump',\n",
       " 'headway',\n",
       " 'periomenopausal',\n",
       " 'rosa',\n",
       " 'yasmin',\n",
       " 'anonymos',\n",
       " 'opentalk',\n",
       " 'lambs',\n",
       " 'shinashi',\n",
       " 'peanutswhich',\n",
       " 'pancham',\n",
       " 'althorp',\n",
       " 'codification',\n",
       " '7950',\n",
       " 'jatbms',\n",
       " 'elastix',\n",
       " 'molotus',\n",
       " 'enoxaparin',\n",
       " 'tovar',\n",
       " 'stitch',\n",
       " '117000',\n",
       " 'instahyre',\n",
       " 'deferent',\n",
       " 'eployee',\n",
       " 'hated',\n",
       " 'mpofu',\n",
       " 'salaman',\n",
       " 'regenerative',\n",
       " 'purpura',\n",
       " 'antetokounmpo',\n",
       " 'scoop',\n",
       " 'airplaine',\n",
       " 'euthyphro',\n",
       " 'alley',\n",
       " 'rahezz',\n",
       " 'livers',\n",
       " 'vandemecum',\n",
       " 'ufeel',\n",
       " 'horde',\n",
       " '22ft',\n",
       " 'madara',\n",
       " 'wenger',\n",
       " 'slaps',\n",
       " 'stonewalling',\n",
       " 'orthoptics',\n",
       " 'conformity',\n",
       " 'pastel',\n",
       " 'imparts',\n",
       " 'strophe',\n",
       " 'auto',\n",
       " 'tenant',\n",
       " 'reid',\n",
       " 'voiceless',\n",
       " 'mgcl',\n",
       " 'ferrus',\n",
       " 'tsukaima',\n",
       " 'muammar',\n",
       " 'chyna',\n",
       " 'cmos',\n",
       " 'steakhouses',\n",
       " '20kgs',\n",
       " 'socialized',\n",
       " 'pasturizer',\n",
       " 'tonthe',\n",
       " 'austrade',\n",
       " 'pleco',\n",
       " 'dynemic',\n",
       " 'andersen',\n",
       " '1406546733',\n",
       " 'actavis',\n",
       " 'whizkid',\n",
       " 'macrocosm',\n",
       " 'adog',\n",
       " 'lawless',\n",
       " 'naccisist',\n",
       " 'tadawul',\n",
       " 'jfk',\n",
       " 'uncharacterized',\n",
       " '45lpa',\n",
       " 'findley',\n",
       " 'féidir',\n",
       " 'aperture',\n",
       " 'farleigh',\n",
       " 'julliard',\n",
       " 'rajarhat',\n",
       " 'options',\n",
       " 'slurring',\n",
       " 'houseowners',\n",
       " 'rodolfo',\n",
       " 'sealand',\n",
       " 'lmax',\n",
       " 'maintenence',\n",
       " 'avbp',\n",
       " 'hexplane',\n",
       " 'loom',\n",
       " 'miti',\n",
       " 'dei',\n",
       " 'quackquack',\n",
       " 'happend',\n",
       " 'c3h8o',\n",
       " 'sufi',\n",
       " 'dst',\n",
       " 'johny',\n",
       " 'chlorthalidone',\n",
       " 'unactivated',\n",
       " 'annihaltion',\n",
       " 'divali',\n",
       " 'iplocation',\n",
       " 'depositing',\n",
       " 'siutation',\n",
       " 'ebooks',\n",
       " 'v90',\n",
       " 'suchitra',\n",
       " 'بعرف',\n",
       " 'exempted',\n",
       " 'markd',\n",
       " 't2i',\n",
       " 'crountries',\n",
       " 'vovinam',\n",
       " 'grout',\n",
       " 'gbm',\n",
       " 'butter',\n",
       " 'advatage',\n",
       " 'nally',\n",
       " 'margaret',\n",
       " 'torontonians',\n",
       " 'highschools',\n",
       " 'miya',\n",
       " '6890',\n",
       " 'godwin',\n",
       " 'mmas',\n",
       " 'mater',\n",
       " 'convo',\n",
       " 'cosc',\n",
       " 'presifted',\n",
       " 'medics',\n",
       " 'pooram',\n",
       " 'turkye',\n",
       " 'misspent',\n",
       " 'microwave',\n",
       " 'apia',\n",
       " 'zibo',\n",
       " 'replay',\n",
       " 'pordon',\n",
       " 'zir',\n",
       " 'tuched',\n",
       " 'periapsis',\n",
       " 'climes',\n",
       " 'panacea',\n",
       " 'fallacies',\n",
       " 'executioners',\n",
       " 'smoothbore',\n",
       " 'nmsqt',\n",
       " 'govindam',\n",
       " '10yo',\n",
       " 'yedyurappa',\n",
       " 'polayt',\n",
       " 'jashan',\n",
       " 'newfindland',\n",
       " 'ddc',\n",
       " 'thermochromic',\n",
       " 'soxhlet',\n",
       " 'freeballing',\n",
       " 'mcdoanlds',\n",
       " 'nif',\n",
       " '2cos1',\n",
       " 'makenzie',\n",
       " 'eibon',\n",
       " 'quickheal',\n",
       " 'hain',\n",
       " 'goat',\n",
       " 'mpod',\n",
       " 'calico',\n",
       " 'pricing',\n",
       " 'marmot',\n",
       " 'maull',\n",
       " 'zstsn2z5kl4',\n",
       " 'eka',\n",
       " 'aget',\n",
       " 'naugat',\n",
       " 'belen',\n",
       " 'twillio',\n",
       " 'gredution',\n",
       " 'heeadphones',\n",
       " 'develops',\n",
       " 'diphteria',\n",
       " 'anteverted',\n",
       " 'naeem',\n",
       " 'anodes',\n",
       " 'domenech',\n",
       " 'hssc',\n",
       " 'inkpad',\n",
       " 'wonk',\n",
       " 'binaries',\n",
       " 'sportiva',\n",
       " 'wackest',\n",
       " 'oppresses',\n",
       " 'scarlet',\n",
       " 'aduls',\n",
       " 'pagodas',\n",
       " 'gangireddu',\n",
       " 'chankya',\n",
       " 'democratized',\n",
       " 'constricting',\n",
       " 'u0247g3',\n",
       " 'mizzone',\n",
       " 'miklo',\n",
       " 'seea',\n",
       " 'schozoid',\n",
       " 'layoff',\n",
       " 'ongoing',\n",
       " 'tutsi',\n",
       " 'gildenlow',\n",
       " 'underwers',\n",
       " 'scrunch',\n",
       " 'moray',\n",
       " 'tocotrienol',\n",
       " 'deindustrialization',\n",
       " '90cm',\n",
       " 'straightening',\n",
       " 'telletubbie',\n",
       " 'finshed',\n",
       " 'singularty',\n",
       " 'softer',\n",
       " 'dettwyler',\n",
       " 'azerbeyjan',\n",
       " 'c0me',\n",
       " 'kao',\n",
       " 'econmically',\n",
       " 'stabbings',\n",
       " 'photochemical',\n",
       " 'h61ms',\n",
       " 'ringside',\n",
       " 'ты',\n",
       " 'popes',\n",
       " 'giovani',\n",
       " 'billionth',\n",
       " 'chicano',\n",
       " 'hyperkinesia',\n",
       " 'exportation',\n",
       " 'homework',\n",
       " 'c3po',\n",
       " 'ghz',\n",
       " 'dwait',\n",
       " 'soyabean',\n",
       " 'giftcard',\n",
       " 'tonacco',\n",
       " 'bukkari',\n",
       " 'quirrel',\n",
       " '04396',\n",
       " 'disfigure',\n",
       " 'walchand',\n",
       " 'clumn',\n",
       " 'sodimm',\n",
       " 'celo',\n",
       " 'tix',\n",
       " 'seibert',\n",
       " 'hourswhile',\n",
       " 'copics',\n",
       " 'hepatocytes',\n",
       " '650sqft',\n",
       " 'chicanos',\n",
       " 'improvoment',\n",
       " 'semester',\n",
       " 'braindumping',\n",
       " 'rav4',\n",
       " 'kinsman',\n",
       " 'dicarboxylic',\n",
       " 'geomentrical',\n",
       " 'aldous',\n",
       " 'temprature',\n",
       " 'ayyo',\n",
       " 'accuweather',\n",
       " 'tansmitted',\n",
       " 'calista',\n",
       " 'inifinity',\n",
       " 'nehlen',\n",
       " '154cm',\n",
       " 'runnels',\n",
       " '17yr',\n",
       " 'fraying',\n",
       " 'dankest',\n",
       " 'metamorphoses',\n",
       " 'guiana',\n",
       " 'registration',\n",
       " 'liquified',\n",
       " 'acquiescence',\n",
       " 'bellytrim',\n",
       " 'glsica',\n",
       " 'consensual',\n",
       " 'bossed',\n",
       " 'iles',\n",
       " 'bang',\n",
       " 'upx',\n",
       " 'delighted',\n",
       " 'netowork',\n",
       " 'stoker',\n",
       " 'qnd',\n",
       " 'presidential',\n",
       " '1840h',\n",
       " 'heyman',\n",
       " 'acquiesced',\n",
       " 'dauphin',\n",
       " 'metalpoint',\n",
       " 'commemorates',\n",
       " 'kibuishi',\n",
       " 'skechers',\n",
       " '18kg',\n",
       " 'workflowmax',\n",
       " 'euclidea',\n",
       " 'agin',\n",
       " 'interparticle',\n",
       " 'attenuated',\n",
       " 'peak',\n",
       " 'adhan',\n",
       " 'chetniks',\n",
       " 'eliminatory',\n",
       " 'unhygeine',\n",
       " 'tmf',\n",
       " '30hz',\n",
       " 'bitwin',\n",
       " 'mybulen',\n",
       " 'languages',\n",
       " 'quors',\n",
       " 'acopalypse',\n",
       " 'malamute',\n",
       " 'huddersfield',\n",
       " 'defiled',\n",
       " 'parity',\n",
       " 'hoping',\n",
       " 'trrs',\n",
       " 'japanese',\n",
       " 'tapan',\n",
       " 'encouragment',\n",
       " 'fsc47',\n",
       " 'nhon',\n",
       " 'splendor',\n",
       " 'staterra',\n",
       " 'wicker',\n",
       " 'inland',\n",
       " '哗众取宠',\n",
       " 'magdelen',\n",
       " 'naharkatia',\n",
       " 'kalk',\n",
       " '25cm',\n",
       " 'seether',\n",
       " '2016x86',\n",
       " 'fiumicino',\n",
       " 'stai',\n",
       " 'amplitude',\n",
       " 'obselete',\n",
       " 'sincromus',\n",
       " 'vod',\n",
       " 'bhuvan',\n",
       " 'diligent',\n",
       " 'lunes',\n",
       " 'waali',\n",
       " 'deseaced',\n",
       " 'inferior',\n",
       " 'fair7',\n",
       " 'backtested',\n",
       " 'accesses',\n",
       " 'slughorn',\n",
       " 'telefonica',\n",
       " 'please',\n",
       " 'struking',\n",
       " 'biginner',\n",
       " 'collctive',\n",
       " 'efraín',\n",
       " 'crisc',\n",
       " 'rajyangam',\n",
       " 'dhisti',\n",
       " 'approve',\n",
       " 'grumble',\n",
       " 'ipl10',\n",
       " 'garudzep',\n",
       " 'woodpecker',\n",
       " 'qiku',\n",
       " 'tvt',\n",
       " 'busses',\n",
       " 'ciliates',\n",
       " 'rockies',\n",
       " 'marrow',\n",
       " 'staionery',\n",
       " 'qaddafi',\n",
       " 'jewing',\n",
       " 'ь',\n",
       " 'davisson',\n",
       " 'barnwal',\n",
       " '464',\n",
       " 'losslessly',\n",
       " 'threshold',\n",
       " 'comparitavely',\n",
       " 'toontown',\n",
       " 'nanoseconds',\n",
       " 'yesmovies',\n",
       " 'quadtrillions',\n",
       " 'bluntness',\n",
       " 'silchar',\n",
       " 'civics',\n",
       " 'orphan',\n",
       " 'whydoes',\n",
       " 'advanvced',\n",
       " 'bangaluru',\n",
       " 'icecaps',\n",
       " 'anxious',\n",
       " 'andalas',\n",
       " 'eager',\n",
       " 'breezy',\n",
       " 'programers',\n",
       " 'abbie',\n",
       " '1th',\n",
       " 'unblemished',\n",
       " 'frustum',\n",
       " 'awoonga',\n",
       " 'addtional',\n",
       " '1776',\n",
       " 'kusumba',\n",
       " 'prep4gre',\n",
       " 'polymorphs',\n",
       " 'nicaraguan',\n",
       " 'metastatis',\n",
       " 'prolog',\n",
       " 'postfish',\n",
       " 'autumnal',\n",
       " 'gotoxy',\n",
       " 'razein',\n",
       " 'monographs',\n",
       " 'rolph',\n",
       " 'solutioin',\n",
       " 'empanadas',\n",
       " 'eductor',\n",
       " 'differant',\n",
       " '75e',\n",
       " 'multicollinearity',\n",
       " 'uxas',\n",
       " 'britsin',\n",
       " 'cinco',\n",
       " 'tyrannically',\n",
       " 'comstock',\n",
       " 'unboxing',\n",
       " 'funnelled',\n",
       " 'countriews',\n",
       " 'perishables',\n",
       " 'buckminsterfullerene',\n",
       " 'eru',\n",
       " 'anticoagulant',\n",
       " 'kitsguru',\n",
       " 'nominee',\n",
       " 'netralaya',\n",
       " 'andalusian',\n",
       " 'governmentof',\n",
       " 'capacitence',\n",
       " 'msic',\n",
       " 'roxbury',\n",
       " 'strife',\n",
       " 'midia',\n",
       " 'skitzo',\n",
       " 'ayutthaya',\n",
       " 'raisal',\n",
       " 'kaingang',\n",
       " 'ragupati',\n",
       " 'nowday',\n",
       " '2qfy16',\n",
       " 'obsoleted',\n",
       " 'unacademy',\n",
       " 'opted',\n",
       " 'bitcion',\n",
       " 'kiwanuka',\n",
       " 'ranches',\n",
       " 'dishonored',\n",
       " 'massine',\n",
       " 'messaged',\n",
       " 'guttural',\n",
       " 'tiffinbox4you',\n",
       " 'defazio',\n",
       " 'panth',\n",
       " 'improvement',\n",
       " '17years',\n",
       " 'lpga',\n",
       " 'kiel',\n",
       " 'caulk',\n",
       " 'uncorrupted',\n",
       " 'rennes',\n",
       " '3000km',\n",
       " 'voxel',\n",
       " 'clutchest',\n",
       " 'todala',\n",
       " 'uplands',\n",
       " 'pharoads',\n",
       " 'huf',\n",
       " 'igive',\n",
       " 'categorising',\n",
       " 'unbore',\n",
       " 'wetted',\n",
       " 'stockpiled',\n",
       " 'agarian',\n",
       " 'quantumobject',\n",
       " 'multipliers',\n",
       " 'anger',\n",
       " 'thsn',\n",
       " 'langan',\n",
       " 'urukk',\n",
       " 'disinfecting',\n",
       " 'ment',\n",
       " 'decalcify',\n",
       " 'nimz',\n",
       " 's4mini',\n",
       " 'rtgs',\n",
       " 'takbirrrr',\n",
       " 'terorists',\n",
       " 'suet',\n",
       " 'migrates',\n",
       " 'manifesting',\n",
       " 'jipmer2017',\n",
       " 'bunau',\n",
       " 'resurrect',\n",
       " 'evaluators',\n",
       " 'lught',\n",
       " 'zunzunwalas',\n",
       " 'descript',\n",
       " 'nstse',\n",
       " 'ne20e',\n",
       " 'syaring',\n",
       " 'wating',\n",
       " 'lamplight',\n",
       " 'paltalk',\n",
       " 'olaf',\n",
       " 'domination',\n",
       " '20910',\n",
       " 'leagly',\n",
       " 'joiners',\n",
       " 'nihilists',\n",
       " 'delinquent',\n",
       " 'wantrank',\n",
       " '7hrs',\n",
       " 'berk',\n",
       " 'reproach',\n",
       " 'utilize',\n",
       " 'handless',\n",
       " 'kekistan',\n",
       " 'balveer',\n",
       " 'intellectul',\n",
       " 'ekological',\n",
       " 'winforms',\n",
       " 'ach',\n",
       " 'transvaginal',\n",
       " 'anthocyanin',\n",
       " 'turmeric',\n",
       " 'xcr',\n",
       " 'holt',\n",
       " 'dystrophy',\n",
       " 'avery',\n",
       " 'ipill',\n",
       " 'breake',\n",
       " 'ovidrel',\n",
       " 'gharials',\n",
       " 'commie',\n",
       " 'oz',\n",
       " 'kahiki',\n",
       " 'parametrizing',\n",
       " 'bandit',\n",
       " 'nlcs',\n",
       " 'abattary',\n",
       " 'halala',\n",
       " 'slayton',\n",
       " 'valastro',\n",
       " 'maul',\n",
       " 'semesters',\n",
       " 'wepower',\n",
       " 'vermin',\n",
       " 'trondheim',\n",
       " 'dalek',\n",
       " 'subversives',\n",
       " 'palvin',\n",
       " 'sevene',\n",
       " 'isopropyl',\n",
       " 'minuscule',\n",
       " 'jcl',\n",
       " 'hmongs',\n",
       " 'cscl2',\n",
       " 'peeiods',\n",
       " 'anatomists',\n",
       " 'crapflooding',\n",
       " 'yalls',\n",
       " 'tweepy',\n",
       " '13009',\n",
       " 'thoroughout',\n",
       " 'shk',\n",
       " 'individuation',\n",
       " 'endowment',\n",
       " 'vaishnavas',\n",
       " 'jacobin',\n",
       " 'burros',\n",
       " 'jannone',\n",
       " 'jorritsma',\n",
       " 'fashioned',\n",
       " 'carpe',\n",
       " 'whonix',\n",
       " 'dous',\n",
       " 'whatifalthist',\n",
       " 'gohana',\n",
       " 'alter',\n",
       " 'whwre',\n",
       " 'amputee',\n",
       " 'kew',\n",
       " 'archaeology',\n",
       " 'cluding',\n",
       " 'apear',\n",
       " 'siling',\n",
       " 'doomfist',\n",
       " '二胡',\n",
       " 'aussi',\n",
       " 'homoeopathy',\n",
       " 'regularisation',\n",
       " 'determines',\n",
       " 'dicta',\n",
       " '55hz',\n",
       " 'coindelta',\n",
       " 'seljuks',\n",
       " '1ma',\n",
       " 'denouncing',\n",
       " 'currics',\n",
       " 'goutam',\n",
       " 'welzel',\n",
       " 'witz',\n",
       " 'adilshah',\n",
       " 'mannerisms',\n",
       " '9261',\n",
       " 'partecipation',\n",
       " 'explicit',\n",
       " 'dane',\n",
       " 'humility',\n",
       " 'haumea',\n",
       " 'tippu',\n",
       " 'zorastrianism',\n",
       " 'epithet',\n",
       " 'exhausts',\n",
       " 'flcl',\n",
       " 'continuum',\n",
       " 'lakshami',\n",
       " '60a',\n",
       " 'northwest',\n",
       " 'lexiscan',\n",
       " 'fatboy',\n",
       " '7m',\n",
       " 'parishioners',\n",
       " 'syntagm',\n",
       " '3543',\n",
       " 'disforic',\n",
       " 'pharmacomodelling',\n",
       " '1styear',\n",
       " 'gnupg',\n",
       " 'vedanta',\n",
       " '4j',\n",
       " 'guruba',\n",
       " 'inhumane',\n",
       " 'unbanned',\n",
       " 'geonee',\n",
       " 'unconscious',\n",
       " 'dextro',\n",
       " 'interpolation',\n",
       " 'gramps',\n",
       " 'trumpov',\n",
       " 'mon',\n",
       " 'meant',\n",
       " 'nondate',\n",
       " 'wetting',\n",
       " 'ragù',\n",
       " 'bespoke',\n",
       " 'saesee',\n",
       " 'intall',\n",
       " 'buddhist',\n",
       " 'brigha',\n",
       " 'engligh',\n",
       " 'treyarch',\n",
       " 'wimberly',\n",
       " '5x',\n",
       " 'facefucking',\n",
       " 'mutilators',\n",
       " 'beachheads',\n",
       " 'godot',\n",
       " 'chiefs',\n",
       " 'hostory',\n",
       " 'mixing',\n",
       " 'portage',\n",
       " 'lto',\n",
       " 'phaeophytisation',\n",
       " 'ucc1',\n",
       " 'cadestral',\n",
       " 'loving',\n",
       " 'americocentric',\n",
       " 'redaction',\n",
       " 'indiginenous',\n",
       " '56mm',\n",
       " 'rosters',\n",
       " 'machale',\n",
       " 'meints',\n",
       " 'proram',\n",
       " 'neaphews',\n",
       " '420s',\n",
       " 'kishinëv',\n",
       " 'mlt',\n",
       " 'fitz',\n",
       " 'uralic',\n",
       " 'aurra',\n",
       " 'dx00',\n",
       " 'merchanting',\n",
       " 'mundo',\n",
       " 'quzhou',\n",
       " 'abiximab',\n",
       " 'e25k',\n",
       " 'twerk',\n",
       " 'hfts',\n",
       " 'azimuth',\n",
       " 'wann',\n",
       " 'destroyed',\n",
       " 'sacrament',\n",
       " 'pin1yin1',\n",
       " 'militarize',\n",
       " 'taco',\n",
       " 'kanthaka',\n",
       " 'hindis',\n",
       " 'iced',\n",
       " 'criticality',\n",
       " 'accelerando',\n",
       " 'fabless',\n",
       " '8gvo9rv2',\n",
       " 'insititutes',\n",
       " 'stupider',\n",
       " 'symphonic',\n",
       " 'imprisoned',\n",
       " 'bsbe',\n",
       " 'battled',\n",
       " 'vfang',\n",
       " 'chloe',\n",
       " 'prigg',\n",
       " 'hillsay',\n",
       " 'dasharath',\n",
       " 'moury',\n",
       " 'intec',\n",
       " 'purose',\n",
       " 'quadrillion',\n",
       " 'nanostructured',\n",
       " 'archaya',\n",
       " 'ruelala',\n",
       " 'gts',\n",
       " 'spears',\n",
       " '2j',\n",
       " 'complicating',\n",
       " 'exoplayer',\n",
       " 'ebat',\n",
       " 'ambushers',\n",
       " 'deforestation',\n",
       " 'aiding',\n",
       " '123lbs',\n",
       " 'reword',\n",
       " 'denys',\n",
       " 'lgbtiq',\n",
       " 'imagesc',\n",
       " 'icann',\n",
       " 'percentilerank',\n",
       " 'derren',\n",
       " 'particpants',\n",
       " 'orchestrate',\n",
       " 'cobblestone',\n",
       " 'cheadle',\n",
       " 'proclination',\n",
       " 'riviver',\n",
       " 'verge',\n",
       " 'steer',\n",
       " 'nibru',\n",
       " 'khushal',\n",
       " 'lactam',\n",
       " 'demange',\n",
       " 'mngl',\n",
       " 'suede',\n",
       " 'jav',\n",
       " 'easyor',\n",
       " 'arisake',\n",
       " 'tritons',\n",
       " 'asbergers',\n",
       " 'whattapp',\n",
       " 'sats',\n",
       " 'decoupage',\n",
       " 'suspicion',\n",
       " 'novalis',\n",
       " 'opine',\n",
       " 'watercress',\n",
       " 'uc4cljr4pnp',\n",
       " 'fuckboy',\n",
       " 'uofalberta',\n",
       " 'autisim',\n",
       " 'mpct',\n",
       " 'robosky',\n",
       " 'harper',\n",
       " 'erole',\n",
       " 'nalada',\n",
       " 'tommy',\n",
       " 'smoot',\n",
       " 'contravariant',\n",
       " 'reorder',\n",
       " 'birma',\n",
       " 'alang',\n",
       " 'gerrity',\n",
       " 'ratemyprofessors',\n",
       " 'pentium',\n",
       " 'vishvanathan',\n",
       " 'stardew',\n",
       " 'fv4034',\n",
       " 'mxp6000',\n",
       " 'meathoda',\n",
       " 'hangouts',\n",
       " 'celeberaties',\n",
       " 'prayanamas',\n",
       " 'mustafar',\n",
       " '89s52',\n",
       " '77500',\n",
       " 'omnivores',\n",
       " 'inflection',\n",
       " 'cryotherapy',\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(filename, vocab):\n",
    "    word2vec = {}\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    f = open(filename, encoding='latin')\n",
    "    for line in tqdm(f):\n",
    "        word, coefs = get_coefs(*line.split(\" \"))\n",
    "        if word in vocab:\n",
    "            word2vec[word] = coefs\n",
    "    f.close()\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram =  'C:/Users/admin/Desktop/NLP/Quora Sent analysis/paragram_300_sl999/paragram_300_sl999.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1703756it [02:28, 11456.21it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec = load_embed(paragram, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134653, (300,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec), word2vec['add'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max words in pos questions: 1, 56\n",
      "min and max words in neg questions: 1, 181\n",
      "min and max words in test questions: 0, 239\n"
     ]
    }
   ],
   "source": [
    "# see the train data closely\n",
    "# min and max number of words in questions\n",
    "lens_t0 = list(map(len, df_t0.processed_text))\n",
    "lens_t1 = list(map(len, df_t1.processed_text))\n",
    "lens_test = list(map(len, test_df.processed_text))\n",
    "print('min and max words in pos questions: {}, {}'.format(min(lens_t0), max(lens_t0)))\n",
    "print('min and max words in neg questions: {}, {}'.format(min(lens_t1), max(lens_t1)))\n",
    "print('min and max words in test questions: {}, {}'.format(min(lens_test), max(lens_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: best 20 frequent word count: [5, 6, 7, 4, 8, 9, 10, 3, 11, 12, 13, 14, 15, 16, 17, 2, 18, 19, 20, 21],  freqs: [105875, 103300, 82300, 80340, 61403, 44480, 32322, 28908, 23904, 17703, 13697, 10374, 7705, 5641, 4113, 3437, 3139, 2446, 1975, 1604],  covers: 99.29%\n",
      "neg: best 20 frequent word count: [6, 7, 5, 8, 9, 10, 4, 11, 12, 13, 14, 15, 16, 17, 3, 18, 19, 20, 21, 22],  freqs: [8138, 7634, 7412, 7019, 6342, 5619, 5325, 5051, 4376, 3822, 3349, 2662, 2154, 1885, 1747, 1628, 1325, 1138, 1011, 855],  covers: 97.13%\n",
      "test: best 20 frequent word count: [5, 6, 7, 4, 8, 9, 10, 3, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20, 21],  freqs: [61140, 59354, 47525, 45416, 35711, 26837, 19582, 16128, 14678, 11060, 8537, 6533, 4978, 3826, 2783, 2172, 1869, 1712, 1365, 1196],  covers: 99.09%\n"
     ]
    }
   ],
   "source": [
    "def freq_stats(tag, counts, key, topk, total):\n",
    "    most_freqs = sorted(counts, key=key, reverse=True)[:topk]\n",
    "    freqs = [counts[freq] for freq in most_freqs]\n",
    "    print('{}: best {} frequent word count: {}, '.format(tag, topk, most_freqs),\n",
    "          'freqs: {}, '.format(freqs),\n",
    "          'covers: {:.2f}%'.format(sum(freqs)/total*100))\n",
    "    return max(most_freqs)\n",
    "\n",
    "from collections import Counter\n",
    "counts_t0 = Counter(lens_t0)\n",
    "counts_t1 = Counter(lens_t1)\n",
    "counts_test = Counter(lens_test)\n",
    "#topk = 5 # vast majority of questions are covered, but may lose clues to classify correctly\n",
    "topk = 20\n",
    "max_t0 = freq_stats('pos', counts_t0, counts_t0.get, topk, SAMPLE_ROWS_T0)\n",
    "max_t1 = freq_stats('neg', counts_t1, counts_t1.get, topk, SAMPLE_ROWS_T1)\n",
    "max_test = freq_stats('test', counts_test, counts_test.get, topk, test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEQ_LENGTH = max(max_t0, max_t1, max_test)\n",
    "SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weights_matrix(word2vec):\n",
    "    word_to_idx = {}\n",
    "    weights_matrix = np.zeros((len(word2vec), 300))\n",
    "    for i, (k, v) in enumerate(word2vec.items()):\n",
    "        word_to_idx[k] = i\n",
    "        weights_matrix[i] = v\n",
    "    return word_to_idx, weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx, weight_matrix = build_weights_matrix(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134653, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of word vector: seq_length\n",
    "def encode_question(word_to_idx, text, seq_length):\n",
    "    encoded = []\n",
    "    for word in text[:seq_length]:\n",
    "        try:\n",
    "            encoded.append(word_to_idx[word])\n",
    "        except KeyError:\n",
    "            # missing words in the table such typos or created words\n",
    "            continue\n",
    "\n",
    "    return np.array(encoded, dtype='int_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds padding\n",
    "def add_padding(numpy_array, seq_length):\n",
    "    cur_length = numpy_array.shape[0]\n",
    "    if cur_length < seq_length:\n",
    "        padding = np.zeros((seq_length-cur_length, ), dtype='int_')\n",
    "        return np.concatenate((padding, numpy_array))\n",
    "    else:\n",
    "        return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(texts, label, word_to_idx, seq_length):\n",
    "    texts_len = len(texts)\n",
    "    y = np.array([label]*texts_len, dtype='float')\n",
    "    X = []\n",
    "    for i, text in enumerate(texts):\n",
    "        text_array = encode_question(word_to_idx, text, seq_length)\n",
    "        text_array = add_padding(text_array, seq_length)\n",
    "        X.append(text_array)\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits train data to train and validation\n",
    "TEST_SIZE = 0.1\n",
    "train_texts_t0, val_texts_t0 = train_test_split(df_t0.processed_text, test_size=TEST_SIZE)\n",
    "train_texts_t1, val_texts_t1 = train_test_split(df_t1.processed_text, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291945    [would, happen, quora, removes, anonymity, fea...\n",
       "526817                [study, cfa, level, 1, mba, doable, ]\n",
       "223523    [messages, passing, three, tiers, architecture...\n",
       "930074    [much, income, get, karnataka, 1000views, indi...\n",
       "789678    [helping, raise, awareness, crimes, help, decr...\n",
       "                                ...                        \n",
       "582669    [get, t3, supplements, without, prescription, ...\n",
       "237908                   [girls, like, black, men, curls, ]\n",
       "798790                           [stockpile, water, home, ]\n",
       "429012                              [best, fiction, read, ]\n",
       "614433    [get, previous, year, question, papers, master...\n",
       "Name: processed_text, Length: 575271, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_t0, train_y_t0 = create_dataset(train_texts_t0, 0, word_to_idx, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_t1, train_y_t1 = create_dataset(train_texts_t1, 1, word_to_idx, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575271, 22)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_t0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train_X (648000, 22), train_y (648000,)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.concatenate((train_X_t0, train_X_t1))\n",
    "train_y = np.concatenate((train_y_t0, train_y_t1))\n",
    "print('shapes: train_X {}, train_y {}'.format(train_X.shape, train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X_t0, val_y_t0 = create_dataset(val_texts_t0, 0, word_to_idx, SEQ_LENGTH)\n",
    "val_X_t1, val_y_t1 = create_dataset(val_texts_t1, 1, word_to_idx, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: val_X (72000, 22), val_y (72000,)\n"
     ]
    }
   ],
   "source": [
    "val_X = np.concatenate((val_X_t0, val_X_t1))\n",
    "val_y = np.concatenate((val_y_t0, val_y_t1))\n",
    "print('shapes: val_X {}, val_y {}'.format(val_X.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "print(torch.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates Tensor datasets\n",
    "train_set = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "val_set = TensorDataset(torch.from_numpy(val_X), torch.from_numpy(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x11e7df69b08>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataloaders\n",
    "# hyperparameter for data loading\n",
    "#  - batch_size: size of one batch\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# make sure to SHUFFLE the training data\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only LSTM (2 or 3 layers) model suffered an overfitting problem.\n",
    "# To avoid the problem, GRU and average pooling layer were added.\n",
    "# The overfitting got better, but still the problem exists.\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, weights, n_out, n_hidden, n_layers,\n",
    "                 bidirectional=False, dropout=0.5, layer_dropout=0.3):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.n_out = n_out\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        if bidirectional:\n",
    "            self.direction = 2\n",
    "        else:\n",
    "            self.direction = 1\n",
    "\n",
    "        num_embeddings, embedding_dim = weights.shape\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # for some reason from_pretrained doesn't work\n",
    "        #self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(weights))\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers,\n",
    "                            batch_first=True, dropout=dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(embedding_dim, n_hidden, n_layers,\n",
    "                          batch_first=True, dropout=dropout,\n",
    "                          bidirectional=bidirectional)\n",
    "        # Conv1d layer\n",
    "        self.conv1d = nn.Conv1d(n_hidden*self.direction, (n_hidden*self.direction)//2, 1)\n",
    "        # Average Pooling layer\n",
    "        self.avp = nn.AvgPool1d(2)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        # Fully-conneted layer\n",
    "        self.fc = nn.Linear((n_hidden*self.direction)//4*2, n_out)\n",
    "        \n",
    "        # Sigmoid activation layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        lstm_hidden, gru_hidden = hidden\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        lstm_out, lstm_hidden = self.lstm(embeds, lstm_hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden*self.direction, seq_len)\n",
    "        lstm_out = self.conv1d(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, seq_len, (self.n_hidden*self.direction)//2)\n",
    "        lstm_out = self.avp(lstm_out)\n",
    "        \n",
    "        gru_out, gru_hidden = self.gru(embeds, gru_hidden)\n",
    "        gru_out = gru_out.contiguous().view(-1, self.n_hidden*self.direction, seq_len)\n",
    "        gru_out = self.conv1d(gru_out)\n",
    "        gru_out = gru_out.contiguous().view(-1, seq_len, (self.n_hidden*self.direction)//2)\n",
    "        gru_out = self.avp(gru_out)\n",
    "        \n",
    "        #out = (lstm_out + gru_out) / 2.0\n",
    "        out = torch.cat((lstm_out, gru_out), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out.float())\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get only last labels\n",
    "        \n",
    "        return sig_out, (lstm_hidden, gru_hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size, bidirectional=False):\n",
    "        weight = next(self.parameters()).data\n",
    "        # for LSTM (initial_hidden_state, initial_cell_state)\n",
    "        lstm_hidden = (\n",
    "            weight.new(self.n_layers*self.direction, batch_size, self.n_hidden).zero_().to(DEVICE),\n",
    "            weight.new(self.n_layers*self.direction, batch_size, self.n_hidden).zero_().to(DEVICE)\n",
    "        )\n",
    "        # for GRU, initial_hidden_state\n",
    "        gru_hidden = weight.new(self.n_layers*self.direction, batch_size, self.n_hidden).zero_().to(DEVICE)\n",
    "        return lstm_hidden, gru_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_out = 1\n",
    "#n_hidden = 512\n",
    "n_hidden = 256\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(134653, 300)\n",
       "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (gru): GRU(300, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (avp): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the network\n",
    "net = SentimentRNN(weight_matrix, n_out, n_hidden, n_layers, bidirectional=False).to(DEVICE)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "#  - lr: learning rate\n",
    "#  - epochs: number of epochs\n",
    "lr = 0.00008\n",
    "epochs = 10\n",
    "clip = 5 # gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "# for now, scheduler is not used. (has a bigger step_size than epochs)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, train_loader, clip, epoch, epochs, gru=True):\n",
    "    # paramters for printing\n",
    "    counter = 0\n",
    "    print_every = 500\n",
    "\n",
    "    train_length = len(train_loader)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = net.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        if gru:\n",
    "            l_h, g_h = hidden\n",
    "            # for LSTM\n",
    "            l_h = tuple([each.data for each in l_h])\n",
    "            # for GRU\n",
    "            g_h = g_h.data\n",
    "            hidden = (l_h, g_h)\n",
    "        else:\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        outputs, hidden = net(inputs, hidden)\n",
    "\n",
    "        # calcuate the loss and perform backprop\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient probelm in RNNs/ LSTMs\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            train_losses.append(loss.item())\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Train Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "                  \"Time: {}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation loss\n",
    "THRESHOLD = 0.6\n",
    "def validate(net, criterion, val_loader, epoch, epochs, gru=True):\n",
    "    hidden = net.init_hidden(BATCH_SIZE)\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            if gru:\n",
    "                val_l_h, val_g_h = hidden\n",
    "                # for LSTM\n",
    "                val_l_h = tuple([each.data for each in val_l_h])\n",
    "                # for GRU\n",
    "                val_g_h = val_g_h.data\n",
    "                hidden = (val_l_h, val_g_h)\n",
    "            else:\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "\n",
    "            outputs, hidden = net(inputs, hidden)\n",
    "            val_loss = criterion(outputs.squeeze(), labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            acc = torch.eq(labels.float(), torch.round(outputs.squeeze())).sum().item()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "              \"Val Acc: {}/{}\".format(acc, BATCH_SIZE),\n",
    "              \"Time: {}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(net,\n",
    "              criterion, optimizer, scheduler,\n",
    "              epochs, train_loader, val_loader,\n",
    "              clip, gru=True):\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        train(net, criterion, optimizer, train_loader, clip, epoch, epochs, gru)\n",
    "        validate(net, criterion, val_loader, epoch, epochs, gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-667f926351bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-073ad6fab9ea>\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(net, criterion, optimizer, scheduler, epochs, train_loader, val_loader, clip, gru)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-ec650814b7ea>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, criterion, optimizer, train_loader, clip, epoch, epochs, gru)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# get the output from the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# calcuate the loss and perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-3829484875bf>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mlstm_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgru_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "run_train(net, criterion, optimizer, scheduler, epochs, train_loader, val_loader, clip, gru=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
